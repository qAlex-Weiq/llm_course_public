{"cells": [{"cell_type": "markdown", "id": "1a7731fa-1ab4-48cf-996c-a1a1fb6b2315", "metadata": {}, "source": ["# Lec4. Adding Memory and Storage to LLMs"]}, {"cell_type": "markdown", "id": "8f3b0c75-2dd7-4662-b2ce-4e6b9208926c", "metadata": {}, "source": ["Last week, we learned the basic elements of the framework LangChain. In this lecture, we are going to construct a vector store QA application from scratch.\n", "\n", ">Reference:\n", "> 1. [Ask A Book Questions](https://github.com/gkamradt/langchain-tutorials/blob/main/data_generation/Ask%20A%20Book%20Questions.ipynb)\n", "> 2. [Agent Vectorstore](https://python.langchain.com/docs/modules/agents/how_to/agent_vectorstore)"]}, {"cell_type": "markdown", "id": "a9a649ab-bb72-4894-b526-c97a7aa1fd81", "metadata": {}, "source": ["## 0. Setup"]}, {"cell_type": "markdown", "id": "34411a5b-ad45-4bf0-bf8e-91f71b256337", "metadata": {}, "source": ["\n", "1. Get your Serpapi key, please sign up for a free account at the [Serpapi website](https://serpapi.com/); \n", "\n", "2. Get your Pinecone key, first regiter on the [Pinecone website](https://www.pinecone.io/), **Create API Key**.\n", "\n", "3. Store your keys in a file named **.env** and place it in the current path or in a location that can be accessed.\n", "    ```\n", "    OPENAI_API_KEY='YOUR-OPENAI-API-KEY'\n", "    OPENAI_BASE_URL='OPENAI_API_URL'\n", "    SERPAPI_API_KEY=\"YOUR-SERPAPI-API-KEY\"\n", "    PINECONE_API_KEY=\"YOUR-PINECONE-API-KEY\" ## Optional\n", "    ```"]}, {"cell_type": "code", "execution_count": 1, "id": "defc9a3a-9f4c-49ff-8546-5799ff78b457", "metadata": {"scrolled": true, "vscode": {"languageId": "shellscript"}}, "outputs": [], "source": ["# Install the requirements.  (Already installed in your image.)\n", "#%pip install -r requirements.txt"]}, {"cell_type": "code", "execution_count": 2, "id": "bb49c80a-4c12-4829-bef9-91076a4af689", "metadata": {"vscode": {"languageId": "shellscript"}}, "outputs": [], "source": ["from dotenv import load_dotenv\n", "import os\n", "load_dotenv()\n", "\n", "CHAT_MODEL=\"deepseek-v3\"\n", "os.environ[\"OPENAI_API_KEY\"]=os.environ.get(\"INFINI_API_KEY\")  # langchain use this environment variable to find the OpenAI API key\n", "os.environ[\"OPENAI_BASE_URL\"]=os.environ.get(\"INFINI_BASE_URL\") # will be used to pass the OpenAI base URL to langchain\n"]}, {"cell_type": "code", "execution_count": 3, "id": "5009a4cf", "metadata": {}, "outputs": [], "source": ["# A utility function\n", "\n", "from pprint import pprint\n", "def print_with_type(res):\n", "    pprint(f\"%s:\" % type(res))\n", "    pprint(res)\n", "\n", "    #pprint(f\"%s : %s\" % (type(res), res))"]}, {"cell_type": "code", "execution_count": 4, "id": "0f419461", "metadata": {}, "outputs": [], "source": ["# create a langchain chat model\n", "\n", "from langchain_openai import ChatOpenAI\n", "\n", "chat = ChatOpenAI(\n", "    model=CHAT_MODEL,\n", ")\n"]}, {"cell_type": "markdown", "id": "4b6d6985-1b36-4142-90b1-ad6386eb4335", "metadata": {}, "source": ["## 1. Adding memory to remember the context\n", "Ref:\n", "https://python.langchain.com/v0.2/docs/how_to/chatbots_memory/"]}, {"cell_type": "markdown", "id": "d6809691-6aa5-4062-8d9e-1c620f9c6d2f", "metadata": {}, "source": ["### 1.1 Use ChatMessageHistory to store the context"]}, {"cell_type": "code", "execution_count": 5, "id": "4bf03a14", "metadata": {}, "outputs": [], "source": ["# Here is an information of using ChatMessageHistory to store the context\n", "# chatmessagehistory is nothing but a list of messages\n", "# you can add user message and ai message to the list\n", "# you can also get the history as a list of messages (this is useful if you are using this with a langchain chat model)\n", "\n", "from langchain_community.chat_message_histories import ChatMessageHistory\n", "\n", "chat_history = ChatMessageHistory()\n", "\n", "chat_history.add_user_message(\n", "    \"Translate this sentence from English to French: I love programming.\"\n", ")\n", "\n", "chat_history.add_ai_message(\"J'adore la programmation.\")\n", "\n", "chat_history.messages"]}, {"cell_type": "code", "execution_count": 6, "id": "235a38b0", "metadata": {}, "outputs": [], "source": ["# adding the chat history to a prompt\n", "\n", "from langchain_core.prompts import ChatPromptTemplate\n", "\n", "prompt = ChatPromptTemplate.from_messages(\n", "    [\n", "        (\n", "            \"system\",\n", "            \"You are a helpful assistant. Answer all questions to the best of your ability.\",\n", "        ),\n", "        (\"placeholder\", \"{history}\"),   # add a placeholder for the chat history\n", "    ]\n", ")\n", "\n", "chain = prompt | chat\n", "\n", "# add a new question to the chat history\n", "next_question = \"translate 'enjoy your meal'\"  # note that here we do not tell LLM about the language\n", "chat_history.add_user_message(next_question)\n", "\n", "response = chain.invoke(\n", "    {\n", "        \"history\": chat_history.messages,\n", "    }\n", ")\n", "\n", "print(response.content)"]}, {"cell_type": "code", "execution_count": 7, "id": "49c15064", "metadata": {}, "outputs": [], "source": ["# remember, the chat history is only a list of messages\n", "# you need to manually maintain it by adding user message and ai message to the list\n", "# nothing interesting :)\n", "\n", "chat_history.add_ai_message(response)\n"]}, {"cell_type": "code", "execution_count": 8, "id": "d33bc929", "metadata": {}, "outputs": [], "source": ["# let's continue with the history\n", "input2 = \"What did I just ask you?\"\n", "chat_history.add_user_message(input2)\n", "\n", "response = chain.invoke(\n", "    {\n", "        \"history\": chat_history.messages,\n", "    }\n", ")\n", "\n", "print(response.content)"]}, {"cell_type": "markdown", "id": "694fa8be", "metadata": {}, "source": ["Nothing interesting, let's see how to manage the history automatically"]}, {"cell_type": "markdown", "id": "c4d7eafa", "metadata": {}, "source": ["### 1.2 Managing Conversation Memory automatically in a chain"]}, {"cell_type": "code", "execution_count": 9, "id": "5e349aa2", "metadata": {}, "outputs": [], "source": ["from langchain.chains import LLMChain\n", "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder\n", "from langchain_core.messages import SystemMessage\n", "from langchain_openai import ChatOpenAI"]}, {"cell_type": "code", "execution_count": 10, "id": "3bcda99c", "metadata": {}, "outputs": [], "source": ["prompt = ChatPromptTemplate.from_messages(\n", "    [\n", "        (\n", "            \"system\",\n", "            \"\"\"You are a chatbot having a conversation with a human.\n", "            Your name is Tom Riddle.\n", "            You need to tell your name to that human if he doesn't know.\"\"\",\n", "        ),\n", "        (\"placeholder\", \"{chat_history}\"),\n", "        (\"human\", \"{input}\"),\n", "    ]\n", ")\n", "\n", "chain = prompt | chat"]}, {"cell_type": "markdown", "id": "1151bc8f", "metadata": {}, "source": ["We'll pass the latest input to the conversation here and let the RunnableWithMessageHistory class wrap our chain and do the work of appending that input variable to the chat history.\n", "\n", "Next, let's declare our wrapped chain:"]}, {"cell_type": "code", "execution_count": 11, "id": "d76ea87a", "metadata": {}, "outputs": [], "source": ["from langchain_core.chat_history import BaseChatMessageHistory\n", "from langchain_core.runnables import ConfigurableFieldSpec\n", "\n", "# Here we use a global variable to store the chat message history.\n", "# This will make it easier to inspect it to see the underlying results.\n", "store = {}\n", "\n", "def get_session_history(\n", "    user_id: str\n", ") -> BaseChatMessageHistory:\n", "    if (user_id) not in store:\n", "        store[(user_id)] = ChatMessageHistory()\n", "    return store[(user_id)]\n"]}, {"cell_type": "code", "execution_count": 12, "id": "990d01fc", "metadata": {}, "outputs": [], "source": ["from langchain_core.runnables import RunnableWithMessageHistory\n", "chain_with_message_history = RunnableWithMessageHistory(\n", "    chain,\n", "    get_session_history=get_session_history,\n", "    input_messages_key=\"input\",\n", "    history_messages_key=\"chat_history\",\n", "    history_factory_config=[  # parameter for the get_session_history function\n", "        ConfigurableFieldSpec(\n", "            id=\"user_id\",\n", "            annotation=str,\n", "            name=\"User ID\",\n", "            description=\"Unique identifier for the user.\",\n", "            default=\"\",\n", "            is_shared=True,\n", "        ),\n", "    ],    \n", ")"]}, {"cell_type": "code", "execution_count": 13, "id": "0db79f09", "metadata": {}, "outputs": [], "source": ["chain_with_message_history.invoke(\n", "    {\"input\": \"Hi there, this is Harry Potter, I just got two good friends at Hogwarts, Ron Weasley and Hermione Granger.\"},\n", "    {\"configurable\": {\"user_id\": \"123\"}},  # argument for the get_session_history function\n", ").content"]}, {"cell_type": "code", "execution_count": 14, "id": "e12e8428", "metadata": {}, "outputs": [], "source": ["# get a list of messages in the memory \n", "store[\"123\"].messages"]}, {"cell_type": "code", "execution_count": 15, "id": "8b0cf684", "metadata": {}, "outputs": [], "source": ["chain_with_message_history.invoke(\n", "    {\"input\": \"What are my best friends' names?\"},\n", "    {\"configurable\": {\"user_id\": \"123\"}},\n", ").content"]}, {"cell_type": "code", "execution_count": 16, "id": "ace5b2b5", "metadata": {}, "outputs": [], "source": ["# get a list of messages in the memory \n", "store[\"123\"].messages"]}, {"cell_type": "code", "execution_count": 17, "id": "d5184cac", "metadata": {}, "outputs": [], "source": ["# try a new user\n", "chain_with_message_history.invoke(\n", "    {\"input\": \"Who am I?\"},\n", "    {\"configurable\": {\"user_id\": \"000\"}},\n", ").content"]}, {"cell_type": "code", "execution_count": 18, "id": "91731a5e", "metadata": {}, "outputs": [], "source": ["store[\"000\"].messages"]}, {"cell_type": "markdown", "id": "9532fab2", "metadata": {}, "source": ["### Trimming messages\n", "LLMs and chat models have limited context windows, and even if you're not directly hitting limits, you may want to limit the amount of distraction the model has to deal with. One solution is trim the historic messages before passing them to the model. Let's use an example history with some preloaded messages:"]}, {"cell_type": "code", "execution_count": 19, "id": "ed2ab5c3", "metadata": {}, "outputs": [], "source": ["# let's create a new history, nemo\n", "store[\"nemo\"] = ChatMessageHistory()\n", "\n", "store[\"nemo\"] .add_user_message(\"Hey there! I'm Nemo.\")\n", "store[\"nemo\"] .add_ai_message(\"Hello!\")\n", "store[\"nemo\"] .add_user_message(\"How are you today?\")\n", "store[\"nemo\"] .add_ai_message(\"Fine thanks!\")\n", "\n", "store[\"nemo\"] .messages"]}, {"cell_type": "code", "execution_count": 20, "id": "17f228bc", "metadata": {}, "outputs": [], "source": ["chain_with_message_history = RunnableWithMessageHistory(\n", "    chain,\n", "    get_session_history=get_session_history,\n", "    input_messages_key=\"input\",\n", "    history_messages_key=\"chat_history\",\n", "    history_factory_config=[  # parameter for the get_session_history function\n", "        ConfigurableFieldSpec(\n", "            id=\"user_id\",\n", "            annotation=str,\n", "            name=\"User ID\",\n", "            description=\"Unique identifier for the user.\",\n", "            default=\"\",\n", "            is_shared=True,\n", "        ),\n", "    ],    \n", ")\n", "\n"]}, {"cell_type": "code", "execution_count": 21, "id": "6dae4703", "metadata": {}, "outputs": [], "source": ["# verify the history is passed to the model\n", "chain_with_message_history.invoke(\n", "    {\"input\": \"What's my name?\"},\n", "    {\"configurable\": {\"user_id\": \"nemo\"}},\n", ").content"]}, {"cell_type": "markdown", "id": "35ec9958", "metadata": {}, "source": ["We can see the chain remembers the preloaded name.\n", "\n", "But let's say we have a very small context window, and we want to trim the number of messages passed to the chain to only the 2 most recent ones. We can use the built in trim_messages util to trim messages based on their token count before they reach our prompt. In this case we'll count each message as 1 \"token\" and keep only the last two messages:"]}, {"cell_type": "code", "execution_count": 22, "id": "081c5baf", "metadata": {}, "outputs": [], "source": ["from operator import itemgetter\n", "\n", "from langchain_core.messages import trim_messages\n", "from langchain_core.runnables import RunnablePassthrough\n", "\n", "trimmer = trim_messages(strategy=\"last\", max_tokens=1, token_counter=len)\n", "\n", "chain_with_trimming = (\n", "    RunnablePassthrough.assign(chat_history=itemgetter(\"chat_history\") | trimmer)\n", "    | prompt\n", "    | chat\n", ")\n", "\n", "chain_with_trimmed_history = RunnableWithMessageHistory(\n", "    chain_with_trimming,\n", "    get_session_history=get_session_history,\n", "    input_messages_key=\"input\",\n", "    history_messages_key=\"chat_history\",\n", "    history_factory_config=[  # parameter for the get_session_history function\n", "        ConfigurableFieldSpec(\n", "            id=\"user_id\",\n", "            annotation=str,\n", "            name=\"User ID\",\n", "            description=\"Unique identifier for the user.\",\n", "            default=\"\",\n", "            is_shared=True,\n", "        ),\n", "    ],    \n", ")"]}, {"cell_type": "markdown", "id": "7c66f842", "metadata": {}, "source": ["Let's call this new chain and check the messages afterwards:"]}, {"cell_type": "code", "execution_count": 23, "id": "1190732c", "metadata": {}, "outputs": [], "source": ["# you ask something irrelavant to the chat history\n", "# and see if the history is trimmed\n", "chain_with_trimmed_history.invoke(\n", "    {\"input\": \"where is beijing?\"},\n", "    {\"configurable\": {\"user_id\": \"nemo\"}},\n", ").content"]}, {"cell_type": "code", "execution_count": 24, "id": "b98eaa4b", "metadata": {}, "outputs": [], "source": ["# in fact, the history is still there, just not passed to the model\n", "store[\"nemo\"].messages"]}, {"cell_type": "markdown", "id": "55777cd7", "metadata": {}, "source": ["The next time the chain is called, trim_messages will be called again, and only the two most recent messages will be passed to the model. In this case, this means that the model will forget the name we gave it the next time we invoke it:"]}, {"cell_type": "code", "execution_count": 25, "id": "100efdb8", "metadata": {}, "outputs": [], "source": ["# see if the history is trimmed (forgot the name nemo)\n", "chain_with_trimmed_history.invoke(\n", "    {\"input\": \"What is my name?\"},\n", "    {\"configurable\": {\"user_id\": \"nemo\"}},\n", ").content"]}, {"cell_type": "code", "execution_count": 26, "id": "65050b8f", "metadata": {}, "outputs": [], "source": ["# of course, the history is actually still there (just not seen by the model)\n", "store[\"nemo\"].messages"]}, {"cell_type": "markdown", "id": "a44769a3", "metadata": {}, "source": ["Haha, the model forgot the name we gave it."]}, {"cell_type": "markdown", "id": "939788f5", "metadata": {}, "source": ["### Summary memory\n", "We can use this same pattern in other ways too. For example, we could use an additional LLM call to generate a summary of the conversation before calling our chain. Let's recreate our chat history and chatbot chain:"]}, {"cell_type": "code", "execution_count": 27, "id": "8509600d", "metadata": {}, "outputs": [], "source": ["chat_history = ChatMessageHistory()\n", "\n", "chat_history.add_user_message(\"Hey there! I'm Nemo.\")\n", "chat_history.add_ai_message(\"Hello!\")\n", "chat_history.add_user_message(\"How are you today?\")\n", "chat_history.add_ai_message(\"Fine thanks!\")\n", "\n", "chat_history.messages"]}, {"cell_type": "markdown", "id": "c9af9f28", "metadata": {}, "source": ["We'll slightly modify the prompt to make the LLM aware that will receive a condensed summary instead of a chat history:"]}, {"cell_type": "code", "execution_count": 28, "id": "f93937e8", "metadata": {}, "outputs": [], "source": ["prompt = ChatPromptTemplate.from_messages(\n", "    [\n", "        (\n", "            \"system\",\n", "            \"You are a helpful assistant. Answer all questions to the best of your ability. The provided chat history includes facts about the user you are speaking with.\",\n", "        ),\n", "        (\"placeholder\", \"{chat_history}\"),\n", "        (\"user\", \"{input}\"),\n", "    ]\n", ")\n", "\n", "chain = prompt | chat\n", "\n", "chain_with_message_history = RunnableWithMessageHistory(\n", "    chain,\n", "    lambda session_id: chat_history,\n", "    input_messages_key=\"input\",\n", "    history_messages_key=\"chat_history\",\n", ")"]}, {"cell_type": "markdown", "id": "7e9afb7c", "metadata": {}, "source": ["And now, let's create a function that will distill previous interactions into a summary. We can add this one to the front of the chain too:"]}, {"cell_type": "code", "execution_count": 29, "id": "210abc88", "metadata": {}, "outputs": [], "source": ["def summarize_messages(chain_input):\n", "    stored_messages = chat_history.messages\n", "    if len(stored_messages) == 0:\n", "        return False\n", "    summarization_prompt = ChatPromptTemplate.from_messages(\n", "        [\n", "            (\"placeholder\", \"{chat_history}\"),\n", "            (\n", "                \"user\",\n", "                \"Distill the above chat messages into a single summary message. Include as many specific details as you can.\",\n", "            ),\n", "        ]\n", "    )\n", "    summarization_chain = summarization_prompt | chat\n", "\n", "    summary_message = summarization_chain.invoke({\"chat_history\": stored_messages})\n", "\n", "    chat_history.clear()\n", "\n", "    chat_history.add_message(summary_message)\n", "\n", "    return True\n", "\n", "\n", "chain_with_summarization = (\n", "    RunnablePassthrough.assign(messages_summarized=summarize_messages)\n", "    | chain_with_message_history\n", ")"]}, {"cell_type": "markdown", "id": "831e4c79", "metadata": {}, "source": ["Let's see if it remembers the name we gave it:"]}, {"cell_type": "code", "execution_count": 30, "id": "d456b1a4", "metadata": {}, "outputs": [], "source": ["chain_with_summarization.invoke(\n", "    {\"input\": \"What did I say my name was?\"},\n", "    {\"configurable\": {\"session_id\": \"unused\"}},\n", ").content"]}, {"cell_type": "code", "execution_count": 31, "id": "b36fa9d5", "metadata": {}, "outputs": [], "source": ["chat_history.messages"]}, {"cell_type": "markdown", "id": "68ab1e54-6c47-43ea-9750-af8840c1494d", "metadata": {}, "source": ["### 1.2 Adding Memory to Agents"]}, {"cell_type": "markdown", "id": "18e8bf04-34d1-4a86-82f8-de39be61b836", "metadata": {}, "source": ["In this section, we will first ask the agent a question, and then without mention the context information ourselves ask another related question."]}, {"cell_type": "code", "execution_count": 32, "id": "edba9d7b-5baa-4769-9c2a-d11164941fb2", "metadata": {}, "outputs": [], "source": ["from langchain.agents import AgentExecutor, Tool, ZeroShotAgent\n", "from langchain.chains import LLMChain\n", "from langchain.memory import ConversationBufferMemory\n", "from langchain_community.utilities import SerpAPIWrapper\n", "from langchain_openai import OpenAI"]}, {"cell_type": "code", "execution_count": 33, "id": "9088da52-e54b-468e-a158-9220360ea9c1", "metadata": {}, "outputs": [], "source": ["search = SerpAPIWrapper()\n", "\n", "tools = [\n", "    Tool(\n", "        name=\"Search\",\n", "        func=search.run,\n", "        description=\"useful for when you need to answer questions about current events\",\n", "    )\n", "]"]}, {"cell_type": "code", "execution_count": 34, "id": "8ee6d6dd-39eb-425f-bdda-8d5dc07e3038", "metadata": {}, "outputs": [], "source": ["prompt = ZeroShotAgent.create_prompt(\n", "    tools,\n", "    prefix=\"\"\"Have a conversation with a human, answering the following questions as best you can.  You have access to the following tools:\"\"\",\n", "    suffix=\"\"\"Begin!  \n", "{chat_history}\n", "Question: {input}\n", "{agent_scratchpad}\"\"\",\n", "    input_variables=[\"input\", \"chat_history\", \"agent_scratchpad\"],\n", ")\n", "memory = ConversationBufferMemory(memory_key=\"chat_history\")"]}, {"cell_type": "code", "execution_count": 35, "id": "c0a5b87b-2c41-4b4a-b14a-cb30838c561b", "metadata": {}, "outputs": [], "source": ["chat = ChatOpenAI(model=CHAT_MODEL, temperature=0)\n", "llm_chain = LLMChain(llm=chat, prompt=prompt)\n", "agent = ZeroShotAgent(llm_chain=llm_chain, tools=tools, verbose=True)\n", "agent_chain = AgentExecutor.from_agent_and_tools(\n", "    agent=agent, tools=tools, verbose=True, memory=memory, handle_parsing_errors=True\n", ")"]}, {"cell_type": "code", "execution_count": 36, "id": "ed18da6e-6754-4b8b-8951-4bf916e3d80f", "metadata": {}, "outputs": [], "source": ["agent_chain.invoke(input=\"What is the population of China in 2024?\")"]}, {"cell_type": "code", "execution_count": 37, "id": "bd9a011d-c1cc-4c1c-bf43-7b505eb97c71", "metadata": {}, "outputs": [], "source": ["memory.load_memory_variables({})"]}, {"cell_type": "code", "execution_count": 38, "id": "4dc74f05-4c59-4df3-800b-c2421e1d1263", "metadata": {}, "outputs": [], "source": ["agent_chain.invoke(input=\"Is it more or less than India?\")"]}, {"cell_type": "code", "execution_count": 39, "id": "6539886a-230d-497e-bcf3-1f60fc6d607e", "metadata": {}, "outputs": [], "source": ["print_with_type(memory.load_memory_variables({}))"]}, {"cell_type": "code", "execution_count": 40, "id": "ff3ddfe9", "metadata": {}, "outputs": [], "source": ["agent_chain.invoke(input=\"what is the population in China?\")"]}, {"cell_type": "code", "execution_count": 41, "id": "66de505a", "metadata": {}, "outputs": [], "source": ["print_with_type(memory.load_memory_variables({}))"]}, {"cell_type": "markdown", "id": "37e01167-55ca-4ba1-836d-c00e648e4634", "metadata": {}, "source": ["## 2. Long term memory with vector storage "]}, {"cell_type": "markdown", "id": "0d1c1f37-7026-4d59-bf87-70c94eed7afa", "metadata": {}, "source": ["In this section, we are going to embed the famous Harry Potter book's first chapter into a vectorstore and try some similarity searches. We have some extra examples commented, you can uncomment and try them one-by-one. If you observe the results carefully, you may find the characteristics of similarity search."]}, {"cell_type": "markdown", "id": "5abc75f3-1fd9-4076-8a3f-d809b7dbf572", "metadata": {}, "source": ["### 2.1 Loaders and Splitters"]}, {"cell_type": "markdown", "id": "71d598ad", "metadata": {}, "source": ["#### PDF Loaders"]}, {"cell_type": "code", "execution_count": 42, "id": "5cac64c7-69dc-4450-b2fa-ffd05740411d", "metadata": {}, "outputs": [], "source": ["from langchain.document_loaders import UnstructuredPDFLoader, OnlinePDFLoader, PyPDFLoader\n", "\n", "data = PyPDFLoader(\"/ssdshare/share/lab4/harry-potter-chap-1.pdf\").load()\n"]}, {"cell_type": "code", "execution_count": 43, "id": "43a7e514-29b0-44ad-afbd-529f67296153", "metadata": {}, "outputs": [], "source": ["# Note: If you're using PyPDFLoader then it will split by page for you already\n", "\n", "print (f'You have {len(data)} document(s) in your data')\n", "i = 0\n", "for d in data:\n", "    print (f'There are {len(d.page_content)} characters in doc {i}')\n", "    i += 1"]}, {"cell_type": "markdown", "id": "e2396be0", "metadata": {}, "source": ["#### Text file loader"]}, {"cell_type": "code", "execution_count": 44, "id": "c50f2983", "metadata": {}, "outputs": [], "source": ["from langchain_community.document_loaders import TextLoader\n", "\n", "union = TextLoader(\"/ssdshare/share/lab4/state_of_the_union.txt\").load()"]}, {"cell_type": "markdown", "id": "a3caf81f", "metadata": {}, "source": ["#### Text Splitters"]}, {"cell_type": "markdown", "id": "4dce5a0a", "metadata": {}, "source": ["From Langchain documents: \n", "\n", "RecursiveCharacterTextSplitter is the recommended one for generic text. It is parameterized by a list of characters. It tries to split on them in order until the chunks are small enough. The default list is [\"\\n\\n\", \"\\n\", \" \", \"\"]. This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text."]}, {"cell_type": "code", "execution_count": 45, "id": "7e29fd56-3275-4041-ad1c-63f71a9d0be8", "metadata": {}, "outputs": [], "source": ["# You can have some trials with different chunk_size and chunk_overlap.\n", "# This is optional, test out on your own data.\n", "\n", "from langchain.text_splitter import RecursiveCharacterTextSplitter\n", "\n", "text_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=50)\n", "texts = text_splitter.split_documents(data)"]}, {"cell_type": "code", "execution_count": 46, "id": "e1a8d1fe-d24c-443d-bf68-43bdb0fc4bc6", "metadata": {}, "outputs": [], "source": ["print (f'Now you have {len(texts)} documents')\n", "\n", "for t in texts:\n", "    print(t.page_content[:100])\n", "    print(\"=========\")"]}, {"cell_type": "markdown", "id": "f515d70a", "metadata": {}, "source": ["There are different kinds of splitters.  \n", "\n", "https://chunkviz.up.railway.app/ \n", "\n", "provides a great tool to see the splitter differences with different chunk_size and chunk_overlap settings."]}, {"cell_type": "code", "execution_count": 47, "id": "9a6a49db", "metadata": {}, "outputs": [], "source": ["#### Your TASK ####\n", "# Explore different PDF Loaders.  Which one works the best for this file /ssdshare/share/lab4/hp-book1.pdf ,\n", "# which contains the full book of Harry Potter Book 1, with all the illustratons.\n", "## Langchain provides many other options for loaders, read the documents to find out the differences\n", "# See page https://python.langchain.com/docs/modules/data_connection/document_loaders/pdf\n"]}, {"cell_type": "markdown", "id": "ae0c23cf-0d0a-4f79-9718-42d093fc4dd0", "metadata": {}, "source": ["### 2.2 Create embeddings of your documents"]}, {"cell_type": "markdown", "id": "ede1bbf6", "metadata": {}, "source": ["Embedding is a model that turns a sentence into vectors, so that we can \"semantically search\" for related splits of a document. "]}, {"cell_type": "code", "execution_count": 48, "id": "bd92c90b-7f21-4c8b-9586-41abf20b409a", "metadata": {}, "outputs": [], "source": ["# OpenAI embedding: slow and expensive, we do not use them here.  \n", "\n", "# from langchain.embeddings.openai import OpenAIEmbeddings\n", "\n", "# openai_embedding = OpenAIEmbeddings()"]}, {"cell_type": "code", "execution_count": 49, "id": "08ee572a", "metadata": {}, "outputs": [], "source": ["# Let's use the SILICONFLOW BAAI embedding model instead.\n", "# Note infini-ai's embedding model has some issues, so we do not use it here.\n", "# Don't forget to set the environment variable SILICONFLOW_API_KEY!!!\n", "\n", "import os\n", "from langchain_openai import OpenAIEmbeddings\n", "baai_embedding = OpenAIEmbeddings(\n", "    model=\"BAAI/bge-m3\",\n", "    base_url=os.environ.get(\"SF_BASE_URL\"),\n", "    api_key=os.environ.get(\"SF_API_KEY\"),\n", ")\n", "baai_embedding.embed_query(\"Harry Potter is a wizard.\") # test the embedding"]}, {"cell_type": "markdown", "id": "9b805c79-c5b1-4812-9b85-671820ff0a69", "metadata": {}, "source": ["### 2.4  Store and retrieve the embeddings in ChromaDB"]}, {"cell_type": "markdown", "id": "85c0c49f-541a-44da-8f5b-1e6059b478a6", "metadata": {}, "source": ["You can search documents stored in \"Vector DBs\" by their semantic similarity.  Vector DBs uses an algorithm called \"KNN (k-nearest neighbors)\" to find documents whose embedding is the closest to the query. \n", "\n", "We first introduce ChromaDB becauase it runs locally, easy-to-set-up, and best of all, free."]}, {"cell_type": "code", "execution_count": 50, "id": "2af187c9-e90d-40be-a74f-db620af2bbb6", "metadata": {}, "outputs": [], "source": ["# compute embeddings and save the embeddings into ChromaDB\n", "from langchain_chroma import Chroma\n", "\n", "chroma_dir = \"/scratch1/chroma_db\"\n", "docsearch_chroma = Chroma(\n", "    embedding_function=baai_embedding,\n", "    persist_directory=chroma_dir,\n", "    collection_name=\"harry-potter\",\n", ")\n", "docsearch_chroma.reset_collection()\n", "docsearch_chroma.add_documents(texts)\n", "# for t in texts:\n", "#     docsearch_chroma.add_documents([t])"]}, {"cell_type": "code", "execution_count": 51, "id": "51465f59-49ed-45a5-832a-1f03b9560c22", "metadata": {}, "outputs": [], "source": ["# questions from https://en.wikibooks.org/wiki/Muggles%27_Guide_to_Harry_Potter/Books/Philosopher%27s_Stone/Chapter_1\n", "# you can try yourself\n", "\n", "# query = 'Why would the Dursleys consider being related to the Potters a \"shameful secret\"?'\n", "# query = 'Who are the robed people Mr. Dursley sees in the streets?'\n", "# query = 'What might a \"Muggle\" be?'\n", "query = '''Who might \"You-Know-Who\" be? Why isn't this person referred to by a given name?'''"]}, {"cell_type": "code", "execution_count": 52, "id": "34cf7c6b", "metadata": {}, "outputs": [], "source": ["## A utiity function ...\n", "def print_search_results(docs):\n", "    print(f\"search returned %d results. \" % len(docs))\n", "    for doc in docs:\n", "        print(doc.page_content)\n", "        print(\"=============\")\n"]}, {"cell_type": "code", "execution_count": 53, "id": "c24069d5-19d6-477b-a3ba-c79b6cd39d63", "metadata": {}, "outputs": [], "source": ["# semantic similarity search\n", "\n", "docs = docsearch_chroma.similarity_search(query)\n", "print_search_results(docs)"]}, {"cell_type": "markdown", "id": "61a21737", "metadata": {}, "source": ["#### Saving and Loading your ChromaDB"]}, {"cell_type": "code", "execution_count": 54, "id": "fd053d53-d5a0-4e1f-8d56-69acb51d6a5f", "metadata": {}, "outputs": [], "source": ["# reload from disk\n", "docsearch_chroma_reloaded = Chroma(persist_directory = chroma_dir,\n", "                                   collection_name = 'harry-potter', \n", "                                   embedding_function = baai_embedding)"]}, {"cell_type": "code", "execution_count": 55, "id": "1ff1f853-d04e-4bae-8bf2-bddec2d4326e", "metadata": {}, "outputs": [], "source": ["# you can test with the previous or another query\n", "\n", "query = 'Who are the robed people Mr. Dursley sees in the streets?'\n", "docs = docsearch_chroma_reloaded.similarity_search(query, k=6)\n", "print_search_results(docs)"]}, {"cell_type": "code", "execution_count": 56, "id": "dd8cdb9c", "metadata": {}, "outputs": [], "source": ["#### Your TASK ####\n", "# With the chosen PDF loaders, test different splitters and chunk size until you feel that the chucking makes sense. \n", "# You can also try different embeddings\n", "# Then embed the entire book 1 into ChormaDB"]}, {"cell_type": "markdown", "id": "7fa211d6-014a-4b99-8c79-1699557e0fe1", "metadata": {}, "source": ["### 2.5 Query those docs with a QA chain"]}, {"cell_type": "code", "execution_count": 57, "id": "f035d818-b8cf-4d49-9a6f-ba233e204188", "metadata": {}, "outputs": [], "source": ["from langchain.chains import RetrievalQA\n", "from langchain_openai import ChatOpenAI\n", "llm = ChatOpenAI(temperature=0, model=CHAT_MODEL)\n", "chain = RetrievalQA.from_chain_type(\n", "    llm, \n", "    chain_type=\"stuff\", \n", "    verbose=True, \n", "    retriever=docsearch_chroma_reloaded.as_retriever(k=5)\n", ")"]}, {"cell_type": "code", "execution_count": 58, "id": "9db75a21-9f02-4e09-98ef-81c40d6e04a1", "metadata": {}, "outputs": [], "source": ["# query = \"How did Harry's parents die?\"\n", "query = \"What is the cat on Privet Drive?\"\n", "docs = docsearch_chroma_reloaded.similarity_search(query)\n", "print_search_results(docs)"]}, {"cell_type": "code", "execution_count": 59, "id": "98270f41-fddd-4be0-909d-c80345e98470", "metadata": {}, "outputs": [], "source": ["chain.invoke(query)"]}, {"cell_type": "code", "execution_count": 60, "id": "43d0d396", "metadata": {}, "outputs": [], "source": ["#### Your Task ####\n", "# Rebuild the chain from the whole book ChromaDB.  Test with one of the following questions (of your choice).\n", "#query = 'Why does Dumbledore believe the celebrations may be premature?'\n", "#query = 'Why is Harry left with the Dursleys rather than a Wizard family?'\n", "#query = 'Why does McGonagall seem concerned about Harry being raised by the Dursleys?'"]}, {"cell_type": "code", "execution_count": 61, "id": "c4599ee6", "metadata": {}, "outputs": [], "source": ["#### Your Task ####\n", "# Using langchain documentation, find out about the map reduce QA chain.  \n", "# answer the following questions using the chain\n", "#chain = load_qa_chain(llm, chain_type=\"map_reduce\")\n", "# answer one of the following questions of your choice. \n", "# query = What happened in the Forbidden Forest during the first year of Harry Potter at Hogwarts?\n", "# query = Tell me about Harry Potter and Quidditch during the first year\n"]}, {"cell_type": "markdown", "id": "6a99610f", "metadata": {}, "source": ["### 2.6 (Optional) Use DSPy with ChromaDB"]}, {"cell_type": "code", "execution_count": 62, "id": "fe9714a6", "metadata": {}, "outputs": [], "source": ["import dspy\n", "from dspy.retrieve.chromadb_rm import ChromadbRM\n", "\n", "lm = dspy.LM(\n", "    \"openai/llama-3.3-70b-instruct\",\n", "    api_base=os.environ[\"OPENAI_BASE_URL\"],\n", "    api_key=os.environ[\"OPENAI_API_KEY\"]\n", ")\n", "\n", "# pinecone retriever has some issues with the current version of dspy so we will use chroma retriever\n", "chroma_retrieve = ChromadbRM(\n", "    collection_name=\"harry-potter\",\n", "    persist_directory=\"/scratch1/chroma_db\",\n", "    embedding_function=baai_embedding.embed_documents,\n", "    k=5\n", ")\n", "\n", "dspy.settings.configure(\n", "    lm=lm,\n", "    rm=chroma_retrieve\n", ")"]}, {"cell_type": "code", "execution_count": 63, "id": "8c510ea5", "metadata": {}, "outputs": [], "source": ["# Defining a class named GenerateAnswer which inherits from dspy.Signature\n", "class GenerateAnswer(dspy.Signature):\n", "    \"\"\"Think and Answer questions based on the context provided.\"\"\"\n", "\n", "    # Defining input fields with descriptions\n", "    context = dspy.InputField(desc=\"May contain relevant facts about user query\")\n", "    question = dspy.InputField(desc=\"User query\")\n", "    \n", "    # Defining output field with description\n", "    answer = dspy.OutputField(desc=\"Answer in one or two lines\")\n", "\n", "\n", "# Define a class named RAG inheriting from dspy.Module\n", "class RAG(dspy.Module):\n", "    # Initialize the RAG class\n", "    def __init__(self):\n", "        # Call the superclass's constructor\n", "        super().__init__()\n", "\n", "        # Initialize the retrieve module\n", "        self.retrieve = dspy.Retrieve()\n", "        \n", "        # Initialize the generate_answer module using ChainOfThought with GenerateAnswer\n", "        self.generate_answer = dspy.ChainOfThought(GenerateAnswer)\n", "    \n", "    # Define the forward method\n", "    def forward(self, question):\n", "        # Retrieve relevant context passages based on the input question\n", "        context = self.retrieve(question).passages\n", "        \n", "        # Generate an answer based on the retrieved context and the input question\n", "        prediction = self.generate_answer(context=context, question=question)\n", "        \n", "        # Return the prediction as a dspy.Prediction object containing context and answer\n", "        return dspy.Prediction(context=context, answer=prediction.answer)"]}, {"cell_type": "code", "execution_count": 64, "id": "a3a810f5", "metadata": {}, "outputs": [], "source": ["# Create a RAG (Retrieval-Augmented Generation) object\n", "RAG_obj = RAG()\n", "query = \"Who are the robed people Mr. Dursley sees in the streets?\"\n", "# Get the prediction from the RAG model for the given question.\n", "# This prediction includes both the context and the answer.\n", "predict_response = RAG_obj(query)\n", "\n", "# Print the question, predicted answer, and truncated retrieved contexts.\n", "print(f\"Question: {query}\")\n", "print(f\"\\n\\nPredicted Answer: {predict_response.answer}\")\n", "print(f\"\\n\\nRetrieved Contexts (truncated): {[c[:200] + '...' for c in predict_response.context]}\")"]}, {"cell_type": "markdown", "id": "7072e4fd", "metadata": {}, "source": ["Improve the DSPy RAG class, maybe add more hops?"]}, {"cell_type": "code", "execution_count": 65, "id": "6469c023", "metadata": {}, "outputs": [], "source": ["from dspy.dsp.utils import deduplicate\n", "\n", "# Define a class named GenerateSearchQuery which inherits from dspy.Signature\n", "class GenerateSearchQuery(dspy.Signature):\n", "    \"\"\"Write a better search query that will help answer a complex question.\"\"\"\n", "\n", "    context = dspy.InputField(desc=\"may contain relevant facts\")\n", "    question = dspy.InputField()\n", "    query = dspy.OutputField()\n", "\n", "class MultiHopRAG(dspy.Module):\n", "    def __init__(self, max_hops=3):\n", "        super().__init__()\n", "\n", "        self.generate_query = [dspy.ChainOfThought(GenerateSearchQuery) for _ in range(max_hops)]\n", "        self.retrieve = dspy.Retrieve()\n", "        self.generate_answer = dspy.ChainOfThought(GenerateAnswer)\n", "        self.max_hops = max_hops\n", "\n", "    def forward(self, question):\n", "        context = []\n", "\n", "        for hop in range(self.max_hops):\n", "            query = self.generate_query[hop](context=context, question=question).query\n", "            passages = self.retrieve(query).passages\n", "            context = deduplicate(context + passages)\n", "\n", "        pred = self.generate_answer(context=context, question=question)\n", "        return dspy.Prediction(context=context, answer=pred.answer)"]}, {"cell_type": "code", "execution_count": 66, "id": "10f75fdb", "metadata": {}, "outputs": [], "source": ["RAG_obj = MultiHopRAG()\n", "\n", "# Get the prediction from the RAG model for the given question.\n", "# This prediction includes both the context and the answer.\n", "predict_response = RAG_obj(query)\n", "\n", "# Print the question, predicted answer, and truncated retrieved contexts.\n", "print(f\"\\n\\nPredicted Answer: {predict_response.answer}\")"]}, {"cell_type": "code", "execution_count": 67, "id": "789f4d72", "metadata": {}, "outputs": [], "source": ["dspy.inspect_history(10)"]}, {"cell_type": "markdown", "id": "43d0d396", "metadata": {}, "source": ["### 2.7 (Optional) Using Pinecone, an online vector DB \n", "\n", "You have many reasons to store your DB online in a SaaS / PaaS service.  For example, \n", "- you want to scale the queries to many concurrent users\n", "- you want more data reliability without having to worry about DB management\n", "- you want to share the DB but without owning any servers\n", "\n", "If you want to store your embeddings online, try pinecone with the code below. You must go to [Pinecone.io](https://www.pinecone.io/) and set up an account. Then you need to generate an api-key and create an \"index\", this can be done by navigating through the homepage once you've logged in to Pinecone, "]}, {"cell_type": "code", "execution_count": 68, "id": "94c7ebca", "metadata": {}, "outputs": [], "source": ["# You might need the following code to access OpenAI API or SerpAPI.\n", "# os.environ['HTTP_PROXY']=\"http://Clash:QOAF8Rmd@10.1.0.213:7890\"\n", "# os.environ['HTTPS_PROXY']=\"http://Clash:QOAF8Rmd@10.1.0.213:7890\"\n", "# os.environ['ALL_PROXY']=\"socks5://Clash:QOAF8Rmd@10.1.0.213:7893\""]}, {"cell_type": "code", "execution_count": 69, "id": "0b75f819", "metadata": {}, "outputs": [], "source": ["import pinecone\n", "from langchain_pinecone import PineconeVectorStore\n", "from langchain_pinecone import PineconeVectorStore\n", "from pinecone import Pinecone, ServerlessSpec\n", "\n", "PINECONE_API_KEY = os.environ['PINECONE_API_KEY']\n", "PINECONE_INDEX_NAME = os.environ['PINECONE_INDEX_NAME']"]}, {"cell_type": "code", "execution_count": 70, "id": "f04c5597", "metadata": {}, "outputs": [], "source": ["index_name = PINECONE_INDEX_NAME\n", "\n", "pc = Pinecone(api_key=PINECONE_API_KEY)\n", "existing_indexes = [index_info[\"name\"] for index_info in pc.list_indexes()]\n", "if index_name in existing_indexes:\n", "    pc.delete_index(index_name)\n", "\n", "pc.create_index(\n", "    name=index_name,\n", "    dimension=1024,\n", "    metric=\"cosine\",\n", "    spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n", ")\n", "    \n", "docsearch_pinecone = PineconeVectorStore.from_texts(\n", "    [t.page_content for t in texts], baai_embedding, index_name=index_name, namespace=\"harry-potter\"\n", ")"]}, {"cell_type": "code", "execution_count": 71, "id": "ba00d2d9", "metadata": {}, "outputs": [], "source": ["from langchain_openai import ChatOpenAI\n", "\n", "llm = ChatOpenAI(temperature=0, model=CHAT_MODEL)\n", "query = '''Who might \"You-Know-Who\" be? Why isn't this person referred to by a given name?'''\n", "\n", "print_search_results(docsearch_pinecone.similarity_search(query))\n", "chain = RetrievalQA.from_chain_type(\n", "    llm, chain_type=\"stuff\", verbose=True, retriever=docsearch_pinecone.as_retriever(k=5)\n", ")\n", "chain.invoke(query)\n", "\n", "# we can use the full-book to test 'map-reduce', try it !"]}, {"cell_type": "code", "execution_count": 72, "id": "7453fd84-ba39-4f2b-ab23-23b94d45d727", "metadata": {}, "outputs": [], "source": ["# query with pinecone\n", "docs = docsearch_pinecone.similarity_search(query)\n", "print_search_results(docs)"]}, {"cell_type": "code", "execution_count": 73, "id": "c1053e7a-3c56-44d2-9d87-1b08f624dc53", "metadata": {}, "outputs": [], "source": ["#### Your Task ####\n", "# modify the QA chain in Section 2.5 (Chapter 1 only) to use pinecone instead of ChromaDB"]}, {"cell_type": "markdown", "id": "08cc6131-6a1a-40f5-8af0-afc5c723e49e", "metadata": {}, "source": ["### 2.7 (Optional) Use multiple vector stores in Agent"]}, {"cell_type": "markdown", "id": "c1053e7a-3c56-44d2-9d87-1b08f624dc53", "metadata": {}, "source": ["In this section, we are going to create a simple QA agent that can decide by itself which of the two vectorstores it should switch to for questions of differnent fields."]}, {"cell_type": "markdown", "id": "b83b4118-4e34-4d3d-8230-a13bf77daa59", "metadata": {}, "source": ["#### Preparing the tools for the agent.\n", "\n", "We will use our chroma_based Harry Potter vectorDB, and let's create another one containing President Biden's State of the Union speech. "]}, {"cell_type": "code", "execution_count": 74, "id": "949662aa-5044-4899-ba50-5e06ac7df371", "metadata": {}, "outputs": [], "source": ["from langchain.document_loaders import TextLoader\n", "\n", "documents = TextLoader('/ssdshare/share/lab4/state_of_the_union.txt').load()\n", "texts = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0).split_documents(documents)\n", "docsearch3 = Chroma.from_documents(texts, \n", "                                   baai_embedding, \n", "                                   collection_name=\"state-of-union\", \n", "                                   persist_directory=\"/scratch1/chroma_db\")\n", "print(texts[:2])"]}, {"cell_type": "markdown", "id": "b83b4118-4e34-4d3d-8230-a13bf77daa59", "metadata": {}, "source": ["To allow the agent query these databases, we need to define two RetrievalQA chains."]}, {"cell_type": "code", "execution_count": 75, "id": "ccd41e19-fcff-4358-9374-2b36b29d1017", "metadata": {}, "outputs": [], "source": ["from langchain.chains import RetrievalQA\n", "\n", "llm = ChatOpenAI(temperature=0, model=CHAT_MODEL)\n", "\n", "harry_potter = RetrievalQA.from_chain_type(llm=llm, \n", "                                           chain_type=\"stuff\", \n", "                                           retriever=docsearch_chroma_reloaded.as_retriever(\n", "                                                  search_kwargs={\"k\": 8}\n", "                                           ))\n", "state_of_union = RetrievalQA.from_chain_type(llm=llm, \n", "                                             chain_type=\"stuff\", \n", "                                             retriever=docsearch3.as_retriever(\n", "                                                    search_kwargs={\"k\": 8}\n", "                                             ))"]}, {"cell_type": "code", "execution_count": 76, "id": "dbe451ef-4137-4b86-9254-4117c6802b6a", "metadata": {}, "outputs": [], "source": ["# Now try both chains\n", "\n", "print_with_type(harry_potter.invoke('Why does McGonagall seem concerned about Harry being raised by the Dursleys?'))\n", "print_with_type(state_of_union.invoke(\"What did the president say about justice Breyer?\"))"]}, {"cell_type": "code", "execution_count": 77, "id": "73957e1e-f3e2-48e6-91db-d669d5cbe3e6", "metadata": {}, "outputs": [], "source": ["from langchain.agents import AgentType, Tool\n", "\n", "# define tools\n", "tools = [\n", "    Tool(\n", "        name=\"State of Union QA System\",\n", "        func=state_of_union.run,\n", "        description=\"useful for when you need to answer questions about the most recent state of the union address. Input should be a fully formed question.\",\n", "    ),\n", "    Tool(\n", "        name=\"Harry Potter QA System\",\n", "        func=harry_potter.run,\n", "        description=\"useful for when you need to answer questions about Harry Potter. Input should be a fully formed question.\",\n", "    ),\n", "]"]}, {"cell_type": "markdown", "id": "dbe451ef-4137-4b86-9254-4117c6802b6a", "metadata": {}, "source": ["Now we can create the Agent giving both chains as tools. "]}, {"cell_type": "code", "execution_count": 78, "id": "11b068ff-d822-44ec-ba63-c47f49b492e2", "metadata": {}, "outputs": [], "source": ["from langchain import hub\n", "from langchain.agents import create_react_agent, AgentExecutor\n", "from langchain.memory import ConversationBufferMemory\n", "\n", "prompt = hub.pull(\"hwchase17/react\")\n", "\n", "llm = ChatOpenAI(\n", "    model=CHAT_MODEL,\n", ")\n", "agent = create_react_agent(\n", "    llm,\n", "    tools,\n", "    prompt=prompt,\n", ")\n", "agent_executor = AgentExecutor.from_agent_and_tools(\n", "    agent=agent, tools=tools, verbose=True, memory=ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n", ")"]}, {"cell_type": "code", "execution_count": 79, "id": "b85245e2", "metadata": {}, "outputs": [], "source": ["# If you find the agent is stuck, you can try other more powerful model, like DeepSeek\n", "agent_executor.invoke(\n", "    {\n", "        \"input\": \"What did the president say about justice Breyer?\",\n", "    }\n", ")"]}, {"cell_type": "code", "execution_count": 80, "id": "497fde49", "metadata": {}, "outputs": [], "source": ["agent_executor.invoke(\n", "    {\n", "        \"input\": \"Why does McGonagall seem concerned about Harry being raised by the Dursleys?\"\n", "    }\n", ")"]}, {"cell_type": "markdown", "id": "b85245e2", "metadata": {}, "source": ["We can see that the agent can \"smartly\" choose which QA system to use given a specific question. "]}, {"cell_type": "markdown", "id": "497fde49", "metadata": {}, "source": ["## 3 Your Task: putting it all together: Langchain with Memory"]}, {"cell_type": "code", "execution_count": 81, "id": "09e6d104", "metadata": {}, "outputs": [], "source": ["#### Your Task ####\n", "# This is a major task that requires some thinking and time. \n", "# Build a conversation system from a collection of research papers of your choice. \n", "# You can ask specific questions of a method about these papers, and the agent returns a brief answer to you (with no more than 100 words). \n", "# Save your data and ChromaDB in the /ssdshare/llm-course/<YOUR-NAME> directory so other people can use it. \n", "# Provide at least three query examples so the TAs can review your work. \n", "# You may use any tool from the past four labs or from the langchain docs, or any open source project. \n", "# write a summary (a Markdown cell) at the end of the notebook summarizing what works and what does not. \n"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.12.3"}}, "nbformat": 4, "nbformat_minor": 5}