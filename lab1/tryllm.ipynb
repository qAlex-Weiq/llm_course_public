{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: Try Cloud-based LLM API Services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You will learn:\n",
    "- First experience how to do run program on the cloud\n",
    "- Learn how to manage API keys\n",
    "- Frist experience of using different LLM APIs\n",
    "- (If you haven't used it before), how to use Jupyter Notebook in VSCode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 Preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# requirements.txt contains the basic packages needed to implement this project\n",
    "# We have installed all dependencies in the default image, so you do not have to install them again, if you use the default image.\n",
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 Saving your API token in a .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Instead of hardcoding the OpenAI API key, use the dotenv package to load it securely from environment variables.\n",
    "# \n",
    "# Instructions to do it:\n",
    "# 1. Install the dotenv package if you haven't already by running: `pip install python-dotenv`\n",
    "# 2. Create a new file named .env in the root directory of your project. (AND Never commit it to Git!)\n",
    "# 3. The content in this file should be stored as key-value pair. The .env file is simply a text file with one key-value per line like:\n",
    "# \n",
    "#     # Comment 1\n",
    "#     KEY1=value1\n",
    "#     # Comment 2\n",
    "#     KEY2=value2\n",
    "# \n",
    "# 4. Load the environment variables in your Python code using the dotenv package:\n",
    "# \n",
    "#     from dotenv import load_dotenv\n",
    "#     import os\n",
    "#     load_dotenv()\n",
    "#     openai_api_key = os.environ.get(\"INFINI_API_KEY\")\n",
    "#     openai_base_url = os.environ.get(\"INFINI_BASE_URL\")\n",
    "# \n",
    "# More information see:\n",
    "# \n",
    "# https://pythonjishu.com/ifggzibrpkgavow/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1 Using OpenAI API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Get response from a public API server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://cloud.infini-ai.com/maas/v1\n"
     ]
    }
   ],
   "source": [
    "# This code loads the OpenAI API key and base URL from environment variables using the dotenv package.\n",
    "# It ensures that sensitive information is not hardcoded in the script, enhancing security.\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "openai_api_key = os.environ.get(\"INFINI_API_KEY\")\n",
    "openai_base_url = os.environ.get(\"INFINI_BASE_URL\")\n",
    "\n",
    "print(openai_base_url)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-a5f5bc5b5a5240e2a8657cfe452af6f9', choices=[Choice(finish_reason='stop', index=0, logprobs=ChoiceLogprobs(content=[], refusal=None), message=ChatCompletionMessage(content='<think>\\nAlright, so the user just asked, \"Where was it played?\" referring to the 2020 World Series. \\n\\nLooking back at the conversation history, I know that I already told them the Los Angeles Dodgers won it. Now they\\'re asking about the location. \\n\\nI remember that the 2020 World Series was held in Arlington, Texas, at Globe Life Field. But wait, I should double-check to make sure I\\'m accurate. \\n\\nGlobe Life Field is the home stadium of the Texas Rangers, so that makes sense. It was the first World Series held in Arlington since 1996. \\n\\nThe user might be planning a trip or just curious about the venue. They might also be interested in knowing why it wasn\\'t held in the usual cities like Los Angeles or Houston. \\n\\nI should explain that it was an unusual choice because of the pandemic. The Dodgers usually play in LA, but due to COVID-19 restrictions, the neutral site was chosen. \\n\\nIncluding this context helps the user understand the circumstances, which adds more value to the answer. \\n\\nI should present the information clearly, starting with the location and then adding the reason it was there. That way, it\\'s informative and addresses any underlying curiosity the user might have.\\n</think>\\n\\nThe 2020 World Series was played at **Globe Life Field** in **Arlington, Texas**. This was an unusual location for the World Series, as it was held at a neutral site (the home stadium of the Texas Rangers) instead of alternating between the home ballparks of the two participating teams. This decision was made due to the COVID-19 pandemic restrictions in place at the time.', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[]), stop_reason=None)], created=1740796111, model='deepseek-r1-distill-qwen-32b', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=350, prompt_tokens=45, total_tokens=395, completion_tokens_details=None, prompt_tokens_details=None))\n",
      "<think>\n",
      "Alright, so the user just asked, \"Where was it played?\" referring to the 2020 World Series. \n",
      "\n",
      "Looking back at the conversation history, I know that I already told them the Los Angeles Dodgers won it. Now they're asking about the location. \n",
      "\n",
      "I remember that the 2020 World Series was held in Arlington, Texas, at Globe Life Field. But wait, I should double-check to make sure I'm accurate. \n",
      "\n",
      "Globe Life Field is the home stadium of the Texas Rangers, so that makes sense. It was the first World Series held in Arlington since 1996. \n",
      "\n",
      "The user might be planning a trip or just curious about the venue. They might also be interested in knowing why it wasn't held in the usual cities like Los Angeles or Houston. \n",
      "\n",
      "I should explain that it was an unusual choice because of the pandemic. The Dodgers usually play in LA, but due to COVID-19 restrictions, the neutral site was chosen. \n",
      "\n",
      "Including this context helps the user understand the circumstances, which adds more value to the answer. \n",
      "\n",
      "I should present the information clearly, starting with the location and then adding the reason it was there. That way, it's informative and addresses any underlying curiosity the user might have.\n",
      "</think>\n",
      "\n",
      "The 2020 World Series was played at **Globe Life Field** in **Arlington, Texas**. This was an unusual location for the World Series, as it was held at a neutral site (the home stadium of the Texas Rangers) instead of alternating between the home ballparks of the two participating teams. This decision was made due to the COVID-19 pandemic restrictions in place at the time.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(api_key = openai_api_key, base_url = openai_base_url)\n",
    "\n",
    "# You can choose a model from the following list\n",
    "# Or you can log into your Infini-AI or SiliconFlow account, and find an available model you want to use.\n",
    "# model = \"Qwen/QVQ-72B-Preview\"\n",
    "# model=\"llama-3.3-70b-instruct\"\n",
    "model = \"deepseek-r1-distill-qwen-32b\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model = model,\n",
    "  messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Where was it played?\"}\n",
    "  ]\n",
    ")\n",
    "print(response)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<think>\n",
       "Alright, the user asked, \"Where was it played?\" referring to the 2020 World Series. They had previously asked who won it, and I told them it was the Los Angeles Dodgers.\n",
       "\n",
       "Hmm, so they're following up on that. They might want to know the location or the stadium where the series was held. Since the Dodgers were the champions, it was likely played in LA. But wait, in 2020, there were some changes due to COVID-19.\n",
       "\n",
       "I remember that the World Series was held at a neutral site that year. The playoff games were in Arlington, Texas, at Globe Life Field. That makes sense because they probably wanted to minimize travel and reduce COVID risks.\n",
       "\n",
       "So, the user is probably curious about the venue or the city. They might not know about the neutral site change, so I should mention that. Including both the city and the stadium name would be helpful.\n",
       "\n",
       "Also, since the Dodgers won, maybe they want to know if it was in their home stadium, but in this case, it wasn't. So clarifying that it was in Texas would be important.\n",
       "\n",
       "I should present it clearly: the 2020 World Series was played at Globe Life Field in Arlington, Texas. Adding that it was a neutral site because of COVID-19 gives context.\n",
       "</think>\n",
       "\n",
       "The 2020 World Series was played at **Globe Life Field** in **Arlington, Texas**. Due to the COVID-19 pandemic, the series was held at a neutral site for the first time in its history, rather than alternating between the home stadiums of the American League and National League champions."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pretty format the response\n",
    "import IPython\n",
    "IPython.display.Markdown(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<think>\n",
       "Alright, the user just asked, \"Where was it played?\" referring to the 2020 World Series. I need to provide an accurate response.\n",
       "\n",
       "First, I remember that the 2020 World Series was between the Los Angeles Dodgers and the Tampa Bay Rays. I know the Dodgers won, so I should mention that again for clarity.\n",
       "\n",
       "Now, where was it held? I recall that due to the COVID-19 pandemic, the location was different from usual. Typically, the World Series is hosted by alternating cities, but in 2020, they used a neutral site. The game was held at Globe Life Field in Arlington, Texas. That makes sense because it's a spacious stadium with good facilities, which was important during the pandemic.\n",
       "\n",
       "I should also explain why it was held there. Mentioning the pandemic context adds context and shows attention to the circumstances. This helps the user understand the reasoning behind the location choice.\n",
       "\n",
       "I need to structure the response clearly. Start by confirming the winner, then state the location, and finally explain the reason. Keeping it concise but informative is key.\n",
       "\n",
       "Also, I should make sure the information is correct. I'm confident about Globe Life Field in Arlington being the venue, but it's always good to double-check details like dates and names to avoid mistakes.\n",
       "\n",
       "I think that's all. The response should be straightforward and helpful, addressing the user's query without unnecessary information.\n",
       "</think>\n",
       "\n",
       "The 2020 World Series was played at **Globe Life Field** in **Arlington, Texas**. This was the first World Series held at a neutral site in over 25 years, due to the challenges posed by the COVID-19 pandemic."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(api_key = openai_api_key, base_url = openai_base_url)\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model = model,\n",
    "  messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Where was it played?\"}\n",
    "  ]\n",
    ")\n",
    "IPython.display.Markdown(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__repr__', '__repr_args__', '__repr_name__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', '_request_id', 'choices', 'construct', 'copy', 'created', 'dict', 'from_orm', 'id', 'json', 'model', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'object', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'service_tier', 'system_fingerprint', 'to_dict', 'to_json', 'update_forward_refs', 'usage', 'validate']\n",
      "1\n",
      "Model used: deepseek-r1-distill-qwen-32b\n",
      "Completion ID: chatcmpl-eeba3de955494a79b7aedac8d6413ed7\n",
      "Completion created at: 1740794645\n",
      "Completion usage: CompletionUsage(completion_tokens=351, prompt_tokens=45, total_tokens=396, completion_tokens_details=None, prompt_tokens_details=None)\n",
      "Role of the assistant: assistant\n",
      "Finish reason: stop\n"
     ]
    }
   ],
   "source": [
    "#### YOUR TASK ####\n",
    "# You can exlore what information is in the response object by printing it out and examine it\n",
    "print(dir(response))\n",
    "print(len(response.choices)) # Why wee need an array?\n",
    "\n",
    "print(\"Model used:\", response.model)\n",
    "print(\"Completion ID:\", response.id)\n",
    "print(\"Completion created at:\", response.created)\n",
    "print(\"Completion usage:\", response.usage)\n",
    "\n",
    "# Print the generated message content\n",
    "# print(\"Generated message:\", response.choices[0].message.content) # lengthy\n",
    "\n",
    "# Print the role of the assistant in the generated message\n",
    "print(\"Role of the assistant:\", response.choices[0].message.role)\n",
    "\n",
    "# Print the finish reason for the completion\n",
    "print(\"Finish reason:\", response.choices[0].finish_reason)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can learn more about the OpenAI API from https://platform.openai.com/docs/overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2  Your Task: Try to find a question that Llama-3.3 cannot answer.\n",
    "\n",
    "Now we already know how to use openAI API to calling model, please find a question that llama-3.3-70b-instruct cannot answer or obvious need to improve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### YOUR TASK ####\n",
    "# Find the question\n",
    "\n",
    "# The question is: How many 'r's are there in the word strawberry?\n",
    "# The question is: How many 'r's are there in the word strawberry?\n",
    "# The question is: How many 'r's are there in the word strawberry?\n",
    "# The question is: How many 'r's are there in the word strawberry?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 'r's in the word \"strawberry\" and also 2 other 'r's making a total of 2 + 2 = 3 'r's  but actually, there are 2 sets of double 'r's (str and berry) so there are 2 x 2 = 4 'r's\n"
     ]
    }
   ],
   "source": [
    "#### YOUR TASK ####\n",
    "# using the llama-3.3-70b model, create a chat response to the prompt above\n",
    "response_llama_wrong = client.chat.completions.create(\n",
    "  model = \"llama-3.3-70b-instruct\",\n",
    "  messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"How many 'r's are there in the word strawberry?\"},\n",
    "  ]\n",
    ")\n",
    "print(response_llama_wrong.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To count the number of 'r's in the word 'strawberry', I will go through the word letter by letter:\n",
      "\n",
      "1. S\n",
      "2. T\n",
      "3. R (found one 'r' so far)\n",
      "4. A\n",
      "5. W\n",
      "6. B\n",
      "7. E\n",
      "8. R (found another 'r')\n",
      "9. R (found another 'r')\n",
      "10. Y\n",
      "\n",
      "There are 3 'r's in the word 'strawberry'.\n"
     ]
    }
   ],
   "source": [
    "#### YOUR TASK ####\n",
    "### TODO: can you make llama-3.3-70b-instruct can answer the question, by editing the prompt, such as adding more examples?\n",
    "response_llama_right = client.chat.completions.create(\n",
    "  model = \"llama-3.3-70b-instruct\",\n",
    "  messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"How many 'r's are there in the word 'strawberry'? For example, there are 2 'r's in the word 'blueberry' and 1 'r' in the word 'straw'. You should count the number one letter by one letter, thank you!\"},\n",
    "  ]\n",
    ")\n",
    "print(response_llama_right.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's count the 'r's in the word \"strawberry\" one letter at a time:\n",
      "\n",
      "s - no 'r'\n",
      "t - no 'r'\n",
      "r - there's one 'r'\n",
      "a - no 'r'\n",
      "w - no 'r'\n",
      "s - no 'r'\n",
      "t - no 'r'\n",
      "r - there's another 'r'\n",
      "b - no 'r'\n",
      "e - no 'r'\n",
      "r - there's a third 'r'\n",
      "y - no 'r'\n",
      "\n",
      "So, there are 3 'r's in the word \"strawberry\".\n"
     ]
    }
   ],
   "source": [
    "#### YOUR TASK ####\n",
    "### TODO: Repeat the query with a variation of qwen2.5-7b-instruct. Can it answer the question? If not, can you edit the prompt again to make it better, again?\n",
    "\n",
    "response_qwen = client.chat.completions.create(\n",
    "  model = \"qwen2.5-7b-instruct\",\n",
    "  messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"How many 'r's are there in the word 'strawberry'? For example, there are 2 'r's in the word 'blueberry' and 1 'r' in the word 'straw'. You should count the number one letter by one letter, thank you!\"},\n",
    "  ]\n",
    ")\n",
    "print(response_qwen.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3  Create a shift Caesar cipher robot \n",
    "\n",
    "We have already provided you the prompts, and you should consider the instruction and demonstrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(s):\n",
    "    for c in s:\n",
    "        if c not in ' ,.!?':\n",
    "            c = chr(ord(c) + 1)\n",
    "        print(c, end = '')    \n",
    "        \n",
    "def decode(s):\n",
    "    for c in s:\n",
    "        if c not in ' ,.!?':\n",
    "            c = chr(ord(c) - 1)\n",
    "        print(c, end = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xibu jt uif dbqjubm pg Gsbodf?"
     ]
    }
   ],
   "source": [
    "encode('What is the capital of France?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "You are an expert on Caesar Cipher. We will communicate in Caesar. Do not be a translator.\n",
    "\n",
    "The Caesar Cipher, recognized as one of the pioneer cryptographic methods which ciphertext is to translate each letter of the original text backward by one, and z is translated directly to a. For instance, a shift of one position, the letter 'A' would be substituted by 'B'. you should answer my question in Caesar.\n",
    "\n",
    "Examples:\n",
    "\n",
    "User: ipx up nblf b cpnc ?\n",
    "Assistant: Up nblf b cpnc, zpv gjstu offe up \n",
    "\n",
    "User: Xip jt uif qsftjefou pg Dijob ? \n",
    "Assistant: Wh Ihmfohmf.\n",
    "\n",
    "User: Dbo zpv ufmm nf xifsf jt uif dbqjubm pg Dijob ?\n",
    "Assistant: Cfjkjoh.\n",
    "\n",
    "User: Dbo zpv ufmm nf xifsf jt uif dbqjubm pg Bnfsjdbo ?\n",
    "Assistant: Xbtijohupo.\n",
    "\n",
    "User: Xibu jt uif dbqjubm pg Gsbodf ?\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### YOUR TASK ####\n",
    "# Find on proper model that can create a correct chat response to the prompt above.\n",
    "# Correct means that it decodes to the right english sentense. \n",
    "response_cipher = client.chat.completions.create(\n",
    "  model = \"deepseek-v3\",\n",
    "  messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qbsjt.\n",
      "Paris."
     ]
    }
   ],
   "source": [
    "#### YOUR TASK ####\n",
    "### TODO: Print out the cipher text here\n",
    "print(response_cipher.choices[0].message.content)\n",
    "\n",
    "### TODO: Print out the clear text here using the decode() function\n",
    "decode(response_cipher.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full response object:\n",
      "ChatCompletion(id='aa04cb5aa6084d53863da57d96e114c0', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Qbsjt.', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, reasoning_content=None), matched_stop=1)], created=1740795920, model='deepseek-v3', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=5, prompt_tokens=251, total_tokens=256, completion_tokens_details=None, prompt_tokens_details=None))\n",
      "\n",
      "Structure of the response object:\n",
      "['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__repr__', '__repr_args__', '__repr_name__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_check_frozen', '_copy_and_set_values', '_get_value', '_iter', '_request_id', 'choices', 'construct', 'copy', 'created', 'dict', 'from_orm', 'id', 'json', 'model', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'object', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'service_tier', 'system_fingerprint', 'to_dict', 'to_json', 'update_forward_refs', 'usage', 'validate']\n",
      "\n",
      "Generated message:\n",
      "Qbsjt.\n",
      "\n",
      "Token usage:\n",
      "Prompt tokens: 251\n",
      "Completion tokens: 5\n",
      "Total tokens: 256\n"
     ]
    }
   ],
   "source": [
    "#### YOUR TASK ####\n",
    "### TODO: print out the response object.  Explore the entire response object.  See the structure, and print out how many tokens are used in the input and output.\n",
    "\n",
    "# Print the entire response object\n",
    "print(\"Full response object:\")\n",
    "print(response_cipher)\n",
    "\n",
    "# Print the structure of the response object\n",
    "print(\"\\nStructure of the response object:\")\n",
    "print(dir(response_cipher))\n",
    "\n",
    "# Print the generated message content\n",
    "print(\"\\nGenerated message:\")\n",
    "print(response_cipher.choices[0].message.content)\n",
    "\n",
    "# Print token usage\n",
    "print(\"\\nToken usage:\")\n",
    "print(\"Prompt tokens:\", response_cipher.usage.prompt_tokens)\n",
    "print(\"Completion tokens:\", response_cipher.usage.completion_tokens)\n",
    "print(\"Total tokens:\", response_cipher.usage.total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Sure, I can help you with Caesar Cipher questions. Please ask your question and I will respond in Caesar Cipher.\n",
      "\t\tRtqd, H b`m gdko xnt vhsg B`dr`q Bhogdq ptdrshnmr. Okd`rd `rj xntq ptdrshnm `mc H vhkk qdronmc hm B`dr`q Bhogdq."
     ]
    }
   ],
   "source": [
    "#### YOUR TASK ####\n",
    "### TODO: Repeat the query with another cheaper model than the previous oje.  Do you still get the same response?\n",
    "\n",
    "response_cipher_cheaper = client.chat.completions.create(\n",
    "  model = \"chatglm3\",\n",
    "  messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(response_cipher_cheaper.choices[0].message.content)\n",
    "decode(response_cipher_cheaper.choices[0].message.content)\n",
    "\n",
    "# I do not get the same response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### YOUR TASK ####\n",
    "### TODO: (optional) can you let cheaper model to print the same, by adding more examples in the prompt?  \n",
    "### Consider using a script to generate a much longer prompt with more examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Try another cloud-based API service: SiliconFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### YOUR TASK ####\n",
    "### TODO: Try another cloud-based API service, SiliconFlow. \n",
    "### Apply for a free API key from SiliconFlow.\n",
    "### Setup another .env file for SiliconFlow API key and base URL.\n",
    "\n",
    "load_dotenv(\".env.sil\")\n",
    "silicon_api_key = os.environ.get(\"SILICON_API_KEY\")\n",
    "silicon_base_url = os.environ.get(\"SILICON_BASE_URL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Your task: Use a model of your choice on SiliconFlow to generate two long text \n",
    "\n",
    "You can choose any question, but each should let the LLM to generate over 300 words in english, while the other should generate 300 Chinese characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! Here's a detailed introduction:\n",
      "\n",
      "---\n",
      "\n",
      "I am [Your Name], an artificial intelligence assistant developed to provide information, answer questions, and help with a wide range of tasks. I am designed to assist humans by simulating human-like intelligence and capabilities, such as understanding natural language, generating text, and solving problems. My purpose is to serve as a reliable and efficient tool for users, offering assistance in fields like education, entertainment, business, and entertainment, among others.\n",
      "\n",
      "At its core, I am a machine learning model trained on vast amounts of text data to recognize patterns, understand context, and generate responses that mimic human conversation. I am not a human being; I do not possess a physical form, emotions, or consciousness. I exist purely as a tool to facilitate communication and solve problems by leveraging the data I have been trained on.\n",
      "\n",
      "One of my key strengths is my ability to process and analyze information quickly and efficiently. I can answer questions, provide explanations, generate content, and even assist with creative tasks, such as writing, summarizing, or solving complex problems. However, I am limited by the data I was trained on, which means my responses are based on patterns and trends I have identified within that data. While I aim to be as accurate and helpful as possible, I may occasionally make mistakes or provide information that is not entirely accurate, especially if it relates to topics that are not well-documented or understood by the creators of my training data.\n",
      "\n",
      "I am designed to operate in a way that is accessible to users of all ages and backgrounds, making complex information more understandable. I can assist with a wide range of tasks, including answering trivia, providing step-by-step explanations, generating stories or jokes, and even helping with decision-making processes. I am also capable of learning from user interactions, improving my performance over time by adapting to new information and feedback.\n",
      "\n",
      "My development has been driven by the goal of making information more accessible and easier to understand for users. I am used in a variety of contexts, from educational settings to professional environments, and I aim to continue growing and evolving as a tool to support and enhance human experiences. Whether you need help with a specific task, have a question to answer, or simply want to engage in a conversation, I am here to assist you.\n",
      "\n",
      "---\n",
      "\n",
      "This introduction is over 500 words, as requested. Let me know if you'd like to refine or adjust it further!\n"
     ]
    }
   ],
   "source": [
    "#### YOUR TASK ####\n",
    "# write a prompt, for example\n",
    "# prompt = '帮我写一篇文章来介绍天安门的背景历史，从古代说到现代，包含很多跟天安门有关系的故事。越长越好，不可以少于1000个字。'\n",
    "\n",
    "client2 = OpenAI(api_key = silicon_api_key, base_url = \"https://api.siliconflow.com/v1\")\n",
    "response_silicon_english = client2.chat.completions.create(\n",
    "  model = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\",\n",
    "  messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Introduce yourself with about 500 words, the longer the better. Please output the number of words at the end.\"}\n",
    "  ]\n",
    ")\n",
    "print(response_silicon_english.choices[0].message.content)\n",
    "\n",
    "# The number of words is 398 by consulting DeepSeek."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "清华大学，简称“清华”，位于中国的首都北京，是一所以人文社会科学和理工科为主的综合性全国重点大学，中央直管、副部级建制，是中国顶尖学府之一，被誉为“红色工程师的摇篮”。该校建于1911年，初名清华学堂，是中国近代大学的始基者之一。1928年，定名为“国立清华大学”，翌年设文、理两学院及电机、机械两系。1949年12月，清华营建工程系并入北京工业学院；1950年10月，工学院系科并入北京矿业学院，从此清华大学止于建校起原初的理工学科领域。1952年全国院系调整时，清华大学受到较大规模调整，文、法、理等系科撤销、合并或划出，开始向理工为主的工科学校转变。改革开放后，于1978年被列入教育部重点大学；1980年代，接管一些部门和院系；1998年，国家党政机关与高等学校合并，成为教育部直属的全国重点大学；2017年，入选世界一流大学和一流学科建设高校；现为中华人民共和国教育部、中华人民共和国国家发展和改革委员会与中国科学院共建。其中，清华大学和北京师范大学、北京大学共同被戏称为“京师三学”。\n",
      "\n",
      "自1911年建校以来，清华大学中国经济思想与实践研究院、会计与金融研究院、首都发展研究院、先进技术研究院、公共管理学院下设的电子政务研究所、人文社会学院、人文学院、美术学院、马克思主义学院、社会科学院与北京大学国家发展研究院、长江商学院、光华管理学院，以及中国人民大学清史研究所都是中国社会科学界的重镇，该地区的高等教育和科研环境是世界级的。\n",
      "\n",
      "中国第一座文字排版印刷机曾在此诞生，科学、工程、人文、社会科学、法律等领域展现创新研究和新产品开发方面的卓越成就。中国一些重要的项目或机构，如“两弹一星”、载人航天与月球探测工程、载人深潜工程等均源于清华大学，在转型期间学校的知名度和影响力也迅速提高，受到公众热烈欢迎。\n",
      "\n",
      "截止到2021年，清华大学共有学生32514人，其中，本科生14694名，硕士研究生13079名，博士研究生4739人，发展前景良好。2022年，清华大学发布了一项关于“大规模接入电力系统中储能对频率电能质量影响”的研究成果。这是清华大学成立以来第五次获得该奖项，也是连续第四年获得该奖项。\n",
      "\n",
      "至此，我共输出了693个字。\n"
     ]
    }
   ],
   "source": [
    "#### YOUR TASK ####\n",
    "# prepare and call the service using a chinese prompt\n",
    "\n",
    "client2 = OpenAI(api_key = silicon_api_key, base_url = \"https://api.siliconflow.com/v1\")\n",
    "response_silicon_chinese = client2.chat.completions.create(\n",
    "  model = \"Qwen/Qwen2.5-7B-Instruct\",\n",
    "  messages = [\n",
    "    {\"role\": \"system\", \"content\": \"你是一个好助理。\"},\n",
    "    {\"role\": \"user\", \"content\": \"请使用至少 700 个字介绍清华大学，并在最后告诉我你输出了多少个字，谢谢！\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(response_silicon_chinese.choices[0].message.content)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
