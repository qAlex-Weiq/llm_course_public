{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Lec3. LangChain\n", "\n", "\n", "**LangChain** is a framework for developing applications powered by language models. It provides abundant abstractions about langage models and sources of context (prompt instructions, few shot examples, content to ground its response in, etc.), which enable the user to easily **chain** these components together for developing awesome applications.\n", "\n", "In this lab, we will learn several key abstractions in LangChain and build an input-output customized AI-powered web-search application.\n", "\n", "### Reference \n", "1. [Langchain document](https://python.langchain.com/docs/introduction/)\n", "\n", "\n", "## 0. First thing first\n", "\n", "### 0.1 Dependencies and Keys\n", "  \n", "In addition to the Open AI keys, add the following keys to your .env file.\n", "\n", "- Serp api key:\n", "    ```\n", "    SERPAPI_API_KEY=\"YOURKEY\"\n", "    ```\n", "    The `SERPAPI_API_KEY` is for invoking the search engine, first register through this [web site](https://serpapi.com/).\n", "\n", "    After getting these two keys, set your keys as environment variables.\n", "- Langchain API key (for tracing)\n", "    ```\n", "    LANGCHAIN_TRACING_V2=\"true\"\n", "    LANGCHAIN_API_KEY=ls_xxxxxxxx\n", "    ```\n", "    To create a `LANGCHAIN_API_KEY`, you can register through the [LANGSMITH](https://docs.smith.langchain.com/)\n", "\n", "\n", "    "]}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [], "source": ["# We have installed these dependencies in your image\n", "#%pip install -r requirements.txt"]}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [], "source": ["from dotenv import load_dotenv  \n", "import os  \n", "\n", "load_dotenv()\n"]}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [], "source": ["import os\n", "# os.environ['HTTP_PROXY']=\"http://Clash:QOAF8Rmd@10.1.0.213:7890\"\n", "# os.environ['HTTPS_PROXY']=\"http://Clash:QOAF8Rmd@10.1.0.213:7890\"\n", "# os.environ['ALL_PROXY']=\"socks5://Clash:QOAF8Rmd@10.1.0.213:7893\""]}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [], "source": ["CHAT_MODEL=\"qwen2.5-72b-instruct\"\n", "os.environ[\"OPENAI_API_KEY\"]=os.environ.get(\"INFINI_API_KEY\")  # langchain use this environment variable to find the OpenAI API key\n", "OPENAI_BASE=os.environ.get(\"INFINI_BASE_URL\") # will be used to pass the OpenAI base URL to langchain"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 1. Key abstractions in LangChain\n", "\n", "| Abstracted Components | Input Type                                | Output Type           |\n", "|-----------------------|-------------------------------------------|-----------------------|\n", "| Prompt                | Dictionary                                | PromptValue           |\n", "| ChatModel             | string, list of messages or a PromptValue | string, ChatMessage   |\n", "| OutputParser          | The output of an LLM or ChatModel         | Depends on the parser |"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 1.1 The ChatModel\n", "\n", "`ChatModels` is a language model which takes a list of messages or a string as input and returns a message or a string.\n", "\n", "`ChatModel` provides two methods to interact with the user:\n", "\n", "- `predict`: takes in a string, returns a string.\n", "- `predict_messages`: takes in a list of messages, returns a message.\n", "\n"]}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [], "source": ["# Create a ChatModel\n", "from langchain_openai import ChatOpenAI\n", "chat_model = ChatOpenAI(\n", "    temperature=0, \n", "    model=CHAT_MODEL,\n", "    base_url=OPENAI_BASE)"]}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [], "source": ["# define an output utility to show the type and the content of the result\n", "def print_with_type(res):\n", "    print(f\"%s\" % (type(res)))\n", "    print(f\"%s\" % res)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["There are four roles in LangChain, and you can define your own custom roles.\n", "\n", "- `HumanMessage`: A ChatMessage coming from a human/user.\n", "- `AIMessage`: A ChatMessage coming from an AI/assistant.\n", "- `SystemMessage`: A ChatMessage coming from the system.\n", "- `FunctionMessage`: A ChatMessage coming from a function call."]}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [], "source": ["from langchain.schema import HumanMessage\n", "\n", "qtext = \"hello! my name is xu wei, nice to meet you! could you tell me something about langchain\"\n", "\n", "messages = []\n", "messages.append(\n", "    HumanMessage(content=qtext)  # construct a human message\n", "    )\n", "res = chat_model.invoke(messages)  # invoke the chat model\n", "\n", "print_with_type(res)\n", "\n", "messages.append(res)  # append the result to the chat history"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The constructors are tedious to use, and you can use the following more friendly API. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# a simpler way to manage messages\n", "from langchain.memory import ChatMessageHistory\n", "history = ChatMessageHistory()\n", "\n", "history.add_user_message(\"hi!\")  # avoid using the constructor directly\n", "history.add_ai_message(\"whats up?\")\n", "history.add_user_message(\"nothing much, you?\")\n", "\n", "res = chat_model.invoke(history.messages)\n", "print_with_type(res)\n"]}, {"cell_type": "code", "execution_count": 9, "metadata": {}, "outputs": [], "source": ["# remembering the chat history and context\n", "\n", "qtext = \"what is its application?\"\n", "messages.append(HumanMessage(content=qtext))  ## providing context of chat histroy\n", "res = chat_model.invoke(messages)\n", "print_with_type(res)\n", "messages.append(res)  ## remembers the histroy"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 1.2 Prompt templates\n", "\n", "LangChain provides ``PromptTemplate`` to help with formatting the prompts. \n", "A ``PromptTemplate`` allows you to define a template with placeholders that can be filled in with specific values at runtime. \n", "This helps in creating dynamic and reusable prompts for different contexts and inputs.\n", "\n", "The most plain prompt is in the type of a ``string``. Usually, the prompt includes several different type of `Messages`, which contains the `role` and the plain prompt as `content`.\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### 1.2.1 Simple template"]}, {"cell_type": "code", "execution_count": 10, "metadata": {}, "outputs": [], "source": ["# Prompt Template\n", "from langchain.prompts import PromptTemplate\n", "\n", "prompt = PromptTemplate.from_template(\"What is a good name for a company that makes {product}?\")\n", "input_prompt = prompt.format(product=\"candies\")\n", "\n", "print_with_type(input_prompt)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### 1.2.2 Chat prompt template"]}, {"cell_type": "code", "execution_count": 11, "metadata": {}, "outputs": [], "source": ["# Chat Template (a list of temlates in a chat prompt template)\n", "\n", "from langchain.prompts.chat import ChatPromptTemplate\n", "\n", "# format chat message prompt\n", "sys_template = \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n", "human_template = \"{text}\"\n", "\n", "chat_prompt = ChatPromptTemplate.from_messages([\n", "    (\"system\", sys_template),\n", "    (\"human\", human_template),\n", "])\n", "chat_input = chat_prompt.format_messages(input_language=\"English\", output_language=\"Chinese\", text=\"I love programming.\")\n", "\n", "print_with_type(chat_input)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### 1.3 Using template in the chat model"]}, {"cell_type": "code", "execution_count": 39, "metadata": {}, "outputs": [], "source": ["# format messages with PromptTemplate with translator as an example\n", "messages = []\n", "chat_input = chat_prompt.format_messages(input_language=\"English\", output_language=\"Chinese\", text=qtext)\n", "print_with_type(chat_input)\n", "print_with_type(chat_model.invoke(chat_input))\n", "\n"]}, {"cell_type": "code", "execution_count": 40, "metadata": {}, "outputs": [], "source": ["messages = chat_input + messages  ## the system message must be at the beginning\n", "print_with_type(messages)\n", "\n", "res = chat_model.invoke(messages)\n", "print_with_type(res)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 1.3 Chaining Components together\n", "\n", "Using an LLM in isolation is fine for simple applications, but more complex applications require chaining LLMs - either with each other or with other components. \n", "In LangChain, most of the above key abstraction components are `Runnable` objects, and we can **chain** them together to build awesome applications. \n", "\n", "LangChain makes the chainning powerful through **LangChain Expression Language (LCEL)**, which can support chainning in manners of:\n", "\n", "- Async, Batch, and Streaming Support: any chain constructed in LCEL can automatically have full synv, async, batch and streaming support. \n", "- Fallbacks: due to many factors like network connection or non-deterministic properties, your LLM applications need to handle errors gracefully. With LCEL, your can easily attach fallbacks any chain.\n", "- Parallelism: since LLM applications involve (sometimes long) API calls, it often becomes important to run things in parallel. With LCEL syntax, any components that can be run in parallel automatically are.\n", "- LangSmith Tracing Integration: (for debugging, see below).\n", "\n", "In this lab, we only demonstrate the simplest functional chainning."]}, {"cell_type": "code", "execution_count": 41, "metadata": {}, "outputs": [], "source": ["# More abstractions: bundling prompt and the chat_model into a chain\n", "\n", "translate_chain = chat_prompt | chat_model\n", "qtext = \"this is input to a chain of chat model and chat prompt.\"\n", "out = translate_chain.invoke({\n", "    \"input_language\": \"English\", \n", "    \"output_language\": \"Chinese\", \n", "    \"text\": {qtext}\n", "    })\n", "print_with_type(out)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 1.4 Output parser\n", "\n", "Language models output text, which is often unstructured and free-form. However, in many applications, you may need more structured information to work with, such as JSON, XML, or other formats. This is where output parsers come in.\n", "\n", "Output parsers are tools that transform the raw text output of language models into structured data formats. The motivation for using output parsers is to facilitate easier data manipulation, integration, and analysis. By converting text into structured formats, you can more effectively utilize the information in downstream applications, automate workflows, and ensure consistency in data handling.\n", "\n", "LangChain provides several commonly-used output parsers, including:\n", "- [JSONparser](https://python.langchain.com/docs/how_to/output_parser_json/): Converts text output into JSON format.\n", "- [XMLparser](https://python.langchain.com/docs/how_to/output_parser_xml/): Converts text output into XML format.\n", "- [YAMLparser](https://python.langchain.com/docs/how_to/output_parser_yaml/): Converts text output into YAML format."]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### 1.4.1 Simple output parser"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# a simple output parser\n", "# StdOutParser converts the chat message to a string.\n", "\n", "from langchain_core.output_parsers import StrOutputParser\n", "output_parser = StrOutputParser()\n", "\n", "stdoutchain = chat_prompt | chat_model | output_parser\n", "\n", "qtext = \"this is input to a chain of chat model and chat prompt.\"\n", "out=stdoutchain.invoke({\n", "    \"input_language\": \"English\", \n", "    \"output_language\": \"Chinese\", \n", "    \"text\": {qtext}\n", "    })\n", "print_with_type(out)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### 1.4.2 Advanced output parsers: from Results to a Python Object\n", "Here we demonstrate the powerful Json Outputparser as an example."]}, {"cell_type": "code", "execution_count": 45, "metadata": {}, "outputs": [], "source": ["from typing import List\n", "from langchain_core.output_parsers import JsonOutputParser\n", "from pydantic import BaseModel, Field\n", "\n", "class Professor(BaseModel):\n", "    name: str = Field(description=\"name of the Professor\")\n", "    publications: str = Field(description=\"the string of the professor's publications, separated by comma\")\n", "\n", "parser = JsonOutputParser(pydantic_object=Professor)\n", "\n", "prompt = PromptTemplate(\n", "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\",\n", "    input_variables=[\"query\"],\n", "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n", ")\n", "\n", "professor_chain = prompt | chat_model | parser\n", "query = \"tell me about professor Wei Xu including his publications.\"\n", "output = professor_chain.invoke({\n", "    \"query\": {query}\n", "    })\n", "print_with_type(output)\n"]}, {"cell_type": "code", "execution_count": 44, "metadata": {}, "outputs": [], "source": ["#### YOUR TASK ####\n", "# see how langchain organizes the input to construct the result.\n", "# you can do so by printing the input of the chat_model.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["You will notice that the list of papers lacks substantial information and contains many inaccuracies. \n", "\n", "This is because the model has no knowlege about Prof. Wei Xu.  \n", "\n", "We will now demonstrate how to address these issues."]}, {"cell_type": "markdown", "metadata": {}, "source": ["# 2. Adding more contexts"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 2.1 Allowing the model to search the web: Retrievers\n", "\n", "Many LLM applications require user-specific data that is not part of the model's training set, like the above example : )\n", "The primary way of accomplishing this is through **Retrieval Augmented Generation (RAG)**. In this process, external data is retrieved and then passed to the LLM when doing the generation step. `Retriever` is an interface that returns documents given an unstructured query, which is used to provide the related contents to LLMs\n", "\n", "LangChain provides all the building blocks for RAG applications - from simple to complex, including document loaders, text embedding models and web searches.  We will introduce these models in Lab 4.  Here, we introduce three very basic retrievers that does web search and some local file access.  \n", "\n", "- [web page](https://python.langchain.com/docs/how_to/document_loader_web/)\n", "- [load Json](https://python.langchain.com/docs/how_to/document_loader_json/)\n", "- [load PDF files](https://python.langchain.com/docs/how_to/document_loader_pdf/)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "SerpAPI is a widely used API to access search engine results. It allows you to scrape and parse search engine data, providing a way to retrieve up-to-date information from the web.\n", "\n", "In LangChain, SerpAPI can be integrated as a retriever to enhance the capabilities of language models by providing them with the latest information from the web. This is particularly useful for applications that require current data that is not part of the model's training set.\n", "\n", "By using SerpAPI with LangChain, you can perform web searches and feed the retrieved data into the language model to generate more accurate and contextually relevant responses.\n", "\n", "In the following sections, we will demonstrate how to set up and use SerpAPI within LangChain to perform web searches and integrate the results into your language model workflows.\n", "\n"]}, {"cell_type": "code", "execution_count": 49, "metadata": {}, "outputs": [], "source": ["# Using the search API\n", "\n", "from langchain.utilities import SerpAPIWrapper\n", "\n", "search = SerpAPIWrapper()\n", "results = search.run(\"Nvidia\")\n", "print_with_type(results)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let's put the search and LLM together."]}, {"cell_type": "code", "execution_count": 50, "metadata": {}, "outputs": [], "source": ["from langchain.schema.runnable import RunnablePassthrough\n", "\n", "class News(BaseModel):\n", "    title: str = Field(description=\"title of the news\")\n", "    brief_desc: str = Field(description=\"brief descrption of the corresponding news\")\n", "\n", "parser = JsonOutputParser(pydantic_object=News)\n", "\n", "prompt = PromptTemplate(\n", "    template=\"Answer the user query based on the following context: \\n{context}\\n{format_instructions}\\nQuery: {query}\",\n", "    input_variables=[\"query\"],\n", "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n", ")\n", "\n", "chat_model.temperature = 0\n", "\n", "search = SerpAPIWrapper()\n", "setup_and_retrieval = {\n", "        \"context\": search.run,  # passing a retriever\n", "        \"query\": RunnablePassthrough()\n", "}\n", "websearch_chain = setup_and_retrieval | prompt | chat_model | parser\n", "\n", "res = websearch_chain.invoke(\"tell me about the nvidia companies, and write a brief summary for it\")\n", "\n", "print_with_type(res)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 2.2 Debugging and Logging\n", "\n", "#### 2.2.1 The debug mode\n", "\n", "LangChain provides a ``debug`` mode that allows you to see detailed information about the execution of your chains and agents. When debug mode is enabled, LangChain will print verbose output showing:\n", "\n", "1. The exact prompts being sent to the LLM\n", "2. The raw responses received from the LLM\n", "3. The execution flow of chains and agents\n", "4. Any intermediate steps and tool calls\n", "\n", "This is extremely useful for:\n", "- Troubleshooting unexpected outputs\n", "- Understanding how your chains are processing data\n", "- Optimizing prompts\n", "- Identifying errors in your chain's logic\n", "\n", "You can enable debug mode using `set_debug(True)` and disable it with `set_debug(False)`.\n"]}, {"cell_type": "code", "execution_count": 56, "metadata": {}, "outputs": [], "source": ["# Debugging and logging: debug mode\n", "from langchain.globals import set_debug\n", "set_debug(True)\n", "\n", "# Try rerun the previous example to see the verbose output.\n", "res = websearch_chain.invoke(\"tell me about the company nvidia, and write a brief summary for it\")\n", "\n", "print_with_type(res)"]}, {"cell_type": "code", "execution_count": 58, "metadata": {}, "outputs": [], "source": ["set_debug(False)\n", "# Try rerun the previous example to see the verbose output.\n", "res = websearch_chain.invoke(\"tell me about the company nvidia, and write a brief summary for it\")\n", "\n", "print_with_type(res)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### 2.2.2 Tracing with LangSmith\n", "\n", "LangSmith is a developer platform that helps you debug, test, evaluate, and monitor LLM applications. It provides:\n", "\n", "1. **Tracing**: Visualize the execution flow of your chains and agents\n", "2. **Debugging**: Inspect inputs, outputs, and intermediate steps\n", "3. **Evaluation**: Measure and compare model performance\n", "4. **Monitoring**: Track usage and performance in production\n", " \n", "To use LangSmith tracing:\n", "Set environment variables (in your .env file):\n", "   - `LANGCHAIN_TRACING_V2=\"true\"` to enable tracing\n", "   - `LANGCHAIN_API_KEY` with your LangSmith API key\n", "\n", "After running your code, you can view detailed traces at https://docs.smith.langchain.com\n"]}, {"cell_type": "code", "execution_count": 23, "metadata": {}, "outputs": [], "source": ["import openai\n", "from langsmith.wrappers import wrap_openai\n", "from langsmith import traceable\n", "\n", "# Auto-trace LLM calls in-context\n", "client = wrap_openai(openai.Client())\n", "\n", "@traceable # Auto-trace this function\n", "def pipeline(user_input: str):\n", "    return websearch_chain.invoke(user_input)\n", "pipeline(\"tell me about the nvidia companies, and write a brief summary for it\")\n", "# Out:  Hello there! How can I assist you today? "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#### your task ####\n", "# go to the langsmith webpage and observe the traces. "]}, {"cell_type": "code", "execution_count": 24, "metadata": {}, "outputs": [], "source": ["#### YOUR TASK ####\n", "# retrieve the information and fix the query results about Prof. Xu, \n", "# generating the correct Professor object.\n", "# Note that you do not have to get a perfect answer from the LLM in this lab.  \n", "# (if the answer is not perfect, please analyze and debug it in the next cell.)\n"]}, {"cell_type": "code", "execution_count": 25, "metadata": {}, "outputs": [], "source": ["#### YOUR TASK ####\n", "# analyze the answer, if the answer is not correct, write down some comments \n", "# about why the answer is not correct. \n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# 3. Smarter workflow: Agents\n", "\n", "\n", "An AI agent is an autonomous system that perceives its environment, makes decisions, and takes actions to achieve specific goals. In the context of LLMs, an agent uses a language model as its reasoning engine to determine what actions to take and in what order, unlike chains where the sequence is predefined.\n", "\n", "LangChain provides several frameworks for building agents:\n", "1. **Tool integration**: LangChain allows agents to use external tools and APIs to gather information or perform actions\n", "2. **Agent types**: Supports various agent architectures like ReAct (Reasoning and Acting), Plan-and-Execute, and others\n", "3. **Memory systems**: Enables agents to maintain context across interactions (next lab)\n", "4. **Structured output**: Helps parse and validate agent responses\n", "\n", "For more advanced agent capabilities, LangGraph (an extension of LangChain) offers enhanced features for creating highly controllable and customizable agents with better state management and complex workflows (next lab).\n", "\n", "\n", "\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 3.1 Function-calling: Letter r's in straberry"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Try the following very simple example, and see if LLM can get it correct. (the correct answer is 3.)"]}, {"cell_type": "code", "execution_count": 69, "metadata": {}, "outputs": [], "source": ["chat_model.invoke(\"how many r's are there in the word strawberry?\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In the example above, the AI answered incorrectly by stating there are two 'r's in the word \"strawberry\" when there are actually three 'r's (st**r**awbe**rr**y). This demonstrates a limitation of LLMs in performing simple counting tasks. Even advanced models can make these basic errors because they process text holistically rather than character-by-character like humans do. This is why tools like function calling are useful - they allow us to delegate specific tasks (like counting) to dedicated functions that can perform them accurately.\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from langchain.agents import tool\n", "\n", "# here is an example of a tool that can be used to count the number of a specific letter in a word.\n", "# note that we only use a single string parameter, because the simple agent only accept tools with a single parameter.\n", "# to fit two parameters, we need a clear format instruction for the input.\n", "@tool\n", "def get_letter_count(query: str) -> int:\n", "    \"\"\"Returns the number of a specific letter in a word. \n", "    The input should be in the format: word,letter (e.g., strawberry,r)\"\"\"\n", "    word, letter = query.split(',')\n", "    return word.count(letter)\n", "\n", "print(get_letter_count.invoke(\"strawberry,r\"))\n", "\n", "tools = [ get_letter_count ]\n", "\n", "print(tools)\n"]}, {"cell_type": "code", "execution_count": 97, "metadata": {}, "outputs": [], "source": ["from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n", "\n", "prompt = ChatPromptTemplate.from_messages(\n", "    [\n", "        ( \"system\", \"You are very powerful assistant who can use tools, but bad at counting letters in words.\", \n", "         ),\n", "        (\"user\", \"{input}\"),\n", "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"), # used to store the previous agent tool invocations and the corresponding tool outputs. \n", "    ]\n", ")"]}, {"cell_type": "code", "execution_count": 98, "metadata": {}, "outputs": [], "source": ["from langchain.agents import initialize_agent\n", "\n", "set_debug(True)\n", "\n", "agent_chain = initialize_agent(tools, \n", "                               chat_model, \n", "                               agent=\"zero-shot-react-description\", \n", "                               prompt_template=prompt, \n", "                               verbose=False\n", "                               )\n", "\n", "agent_chain.invoke({\"input\": \"how many r's are there in the word strawberry?\"})\n", "\n", "set_debug(False)"]}, {"cell_type": "code", "execution_count": 99, "metadata": {}, "outputs": [], "source": ["#### your task ####\n", "# implement a tool that sort an array of numbers (packed as a comma separated string)\n", "# then ask the agent to use the tool to sort an input array. \n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 3.2 Create an auto-web-search AI Agent\n"]}, {"cell_type": "code", "execution_count": 101, "metadata": {}, "outputs": [], "source": ["# read the following example about the built-in web-search tool and understand the code. \n", "\n", "\n", "from langchain.agents import load_tools \n", "\n", "parser = JsonOutputParser(pydantic_object=News)\n", "prompt = ChatPromptTemplate.from_messages(\n", "    [\n", "        (\n", "            \"system\",\n", "            \"You are very powerful assistant, helping the users search the web and write summary for the user's interested topic: {keyword}\",\n", "        ),\n", "        (\"user\", \"{keyword}\"),\n", "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"), # used to store the previous agent tool invocations and the corresponding tool outputs. \n", "    ]\n", ")\n", "\n", "\n", "tools = [load_tools([\"serpapi\"], chat_model)[0]]\n", "\n", "agent_chain = initialize_agent(tools, chat_model, agent=\"zero-shot-react-description\", prompt_template=prompt, verbose=True)"]}, {"cell_type": "code", "execution_count": 102, "metadata": {}, "outputs": [], "source": ["agent_chain.invoke(\"tell me the news from tsinghua university within last week?\")"]}, {"cell_type": "code", "execution_count": 32, "metadata": {}, "outputs": [], "source": ["#### YOUR TASK ####\n", "# use the web search tool to find out about \n", "# prof. wei xu and his publication list.  \n", "# Compare the results with the previous implementation. is it better or worse?\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 3.3 Use one of the built-in tool for langchain "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#### your task ####\n", "# use a built-in tool in langchain to complete a task of your choice. \n", "# in the comment, please describe the task and the tool you used. \n", "# optional: try to use more than one tool in the same agent"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.12.3"}}, "nbformat": 4, "nbformat_minor": 2}